{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a script you can use and modify to see what your historical/recent AO3 activity has been. It keeps your login credentials local to your computer, so you don't have to worry about compromising your account. You can select whether to look at all your history or just what you've bookmarked, and it will tell you what your most commonly read freeform tags, characters, and relationships are. It will also tell you the total and average wordcount of fics you've read in the set you select. If some freeform tags should logically be grouped together (e.g., \"Slow Burn\" means the same thing as \"Slooooow Burn\"), you can do that, and if you'd like to ignore some tags, you can do that also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You will need to fill in a few pieces of information, specifically the headers and your username. Everything else you can leave as-is, though you can also modify it if you want. \n",
    "\n",
    "### Here are instructions on how to get the headers: https://stackoverflow.com/questions/23102833/how-to-scrape-a-website-which-requires-login-using-python-and-beautifulsoup\n",
    "\n",
    "### Tl;dr\n",
    "1. In your browser, open the developer tools or inspect mode\n",
    "2. Go to AO3's website and login\n",
    "3. After the login, go to the network tab, and then refresh the page\n",
    "4. Right click the site request (the top one), hover over copy, and then copy as cURL\n",
    "5. Then go to this site which converts cURL into python requests: https://curl.trillworks.com/\n",
    "6. Add 'bot' to the end of the user-agent per AO3's request\n",
    "7. Take the python code and use the generated headers to proceed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill this parameter according to the directions above\n",
    "# the formatting should look a bit like what's below but more filled in\n",
    "# please also add 'bot' to the end of your user-agent string to play nice with AO3\n",
    "# https://archiveofourown.org/admin_posts/18804\n",
    "headers = {\n",
    "    'authority': 'archiveofourown.org',\n",
    "    'cache-control': '',\n",
    "    'sec-ch-ua': '',\n",
    "    'sec-ch-ua-mobile': '',\n",
    "    'sec-ch-ua-platform': '',\n",
    "    'upgrade-insecure-requests': '',\n",
    "    'user-agent': '',\n",
    "    'accept': '',\n",
    "    'sec-fetch-site': '',\n",
    "    'sec-fetch-mode': '',\n",
    "    'sec-fetch-user': '',\n",
    "    'sec-fetch-dest': '',\n",
    "    'referer': 'https://archiveofourown.org/users/',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cookie': '',\n",
    "} \n",
    "\n",
    "username = 'octocombo' # put your username here\n",
    "num_pages_bookmark = 5 # the number of pages of your bookmarks you want to scrape, starting with the most recent\n",
    "num_pages_hist = 15 # the number of pages of your full history you want to scrape, starting with the most recent\n",
    "num_tags_wanted = 25 # number of top freeform tag groups you want to see\n",
    "\n",
    "# define any tags you consider equivalent that don't necessarily have a great regular expression to group together\n",
    "# this is case-sensitive\n",
    "equivalencies = [['Specific Tag Number 1', 'Similar Tag Number 2', 'Related Tag Number 3'],\n",
    "                 ['First Time','First Kiss','Loss of Virginity','First Love'],\n",
    "                 ] \n",
    "\n",
    "# define any tags you consider equivalent and would want to see grouped that do have a decent regular expression\n",
    "# this is not case-sensitive\n",
    "stem_list = ['.*the period means any character, the asterisk means zero or more times.*', \n",
    "             '(put things in parentheses with a pipe in between)|(if there are a few options)',\n",
    "             '.*Slow B.*', '(.*Non-Canon.*)|(.*Crack.*)',\n",
    "             ]\n",
    "\n",
    "# define specific tags to ignore that are just very common tags or tags you don't care about the frequency of\n",
    "# this is case-sensitive\n",
    "blacklist = ['Canon-Typical Violence','Getting Together','Fluff and Smut',\n",
    "            'Fluff',]\n",
    "\n",
    "# define regular expression tags to ignore because they're not meaningful to you\n",
    "# this is not case-sensitive\n",
    "stemblacklist = ['(.*Alternate Universe.*)|(.* AU.*)', '.*No Beta.*',]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Imports and class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "delay = 5\n",
    "\n",
    "class ao3_tag:\n",
    "    \n",
    "    def __init__(self, c=1, t=''):\n",
    "        self.count = c\n",
    "        self.text = t\n",
    "        self.equivs = [self.text]\n",
    "        for i in range(len(equivalencies)):\n",
    "            if self.text in equivalencies[i]:\n",
    "                self.equivs = equivalencies[i]\n",
    "\n",
    "    def get_data(self):\n",
    "        print(f'{self.count} - {self.text}')\n",
    "\n",
    "    def get_count(self):\n",
    "        return self.count\n",
    "\n",
    "    def get_text(self):\n",
    "        return self.text\n",
    "\n",
    "    def add_text(self,t=''):\n",
    "        self.text = self.text+t\n",
    "\n",
    "    def get_equivs(self):\n",
    "        return self.equivs\n",
    "\n",
    "    def increment(self, adder=1):\n",
    "        self.count = self.count + adder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect URLs to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bookmarks urls\n",
    "# Run this code block if you want the results to be from only the fics you have bookmarked\n",
    "tag_list = []\n",
    "stem_add_list = [[] for i in range(len(stem_list))] \n",
    "stem_add_blacklist = []\n",
    "stem_counts = [0] * len(stem_list)\n",
    "\n",
    "bookmark0 = 'https://archiveofourown.org/users/'+username+'/bookmarks'\n",
    "bookmarkx = 'https://archiveofourown.org/users/'+username+'/bookmarks?page=' \n",
    "urls = [bookmark0]\n",
    "for i in range(num_pages_bookmark-1):\n",
    "    item_next = bookmarkx + str(i+2)\n",
    "    urls.append(item_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# History urls\n",
    "# Run this code block if you want the results to be from all your recent history \n",
    "# If you run this code block after running the bookmark block, you will get your full history, not just bookmarks\n",
    "tag_list = []\n",
    "stem_add_list = [[] for i in range(len(stem_list))] \n",
    "stem_add_blacklist = []\n",
    "stem_counts = [0] * len(stem_list)\n",
    "\n",
    "history0 = 'https://archiveofourown.org/users/'+username+'/readings'\n",
    "historyx = 'https://archiveofourown.org/users/'+username+'/readings?page='\n",
    "urls = [history0]\n",
    "for i in range(num_pages_hist-1):\n",
    "    item_next = historyx + str(i+2)\n",
    "    urls.append(item_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeform tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The output of this cell will be the total number of tags found in the set of fics you looked at\n",
    "for u in urls:\n",
    "    time.sleep(delay)\n",
    "    response = requests.get(u, headers = headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    job_elements = soup.find_all(\"ul\", class_=\"tags commas\") \n",
    "    \n",
    "    for j in job_elements:\n",
    "        tags = j.find_all(\"li\", class_=\"freeforms\") \n",
    "        \n",
    "        for t in tags:\n",
    "            tx = t.text\n",
    "            if tx not in blacklist:\n",
    "                found = False\n",
    "                for i in range(len(stem_list)):\n",
    "                    if re.match(stem_list[i], tx, re.IGNORECASE) is not None:\n",
    "                        found = True\n",
    "                        if tx not in stem_add_list[i]:\n",
    "                            stem_add_list[i].append(tx)\n",
    "                        stem_counts[i] = stem_counts[i] + 1\n",
    "                if not found:\n",
    "                    for i in range(len(stemblacklist)):\n",
    "                        if re.match(stemblacklist[i], tx, re.IGNORECASE) is not None:\n",
    "                            found = True\n",
    "                            if tx not in stem_add_blacklist:\n",
    "                                stem_add_blacklist.append(tx)\n",
    "                if not found:\n",
    "                    for i in range(len(tag_list)):\n",
    "                        if tx in tag_list[i].get_equivs():\n",
    "                            tag_list[i].increment()\n",
    "                            found = True\n",
    "                if not found:\n",
    "                    new_tag = ao3_tag(t=tx)\n",
    "                    tag_list.append(new_tag)\n",
    "\n",
    "print(len(tag_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get results\n",
    "# The number is how many times the tag or tag group appeared in fics you read\n",
    "# If you have groupings, everything that was included in that grouping will be listed together\n",
    "# If you find this list includes a lot of tags you don't care about, add them to the blacklist and re-run\n",
    "# If you find this list has a lot of similar tags, you can group them in the initial parameters and re-run\n",
    "# These will be sorted from most common to least common, if there's a tie the sorting is arbitrary\n",
    "# The number of results in this list is equal to the num_tags_wanted parameter you set at the start\n",
    "\n",
    "for i in range(len(stem_list)):\n",
    "    new_tag = ao3_tag(t=stem_add_list[i],c=stem_counts[i])\n",
    "    tag_list.append(new_tag)\n",
    "sortedlist = sorted(tag_list, key=lambda ao3_tag: ao3_tag.count, reverse = True)\n",
    "\n",
    "for i in range(num_tags_wanted):\n",
    "    sortedlist[i].get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you want to check that your regex blacklist isn't filtering out things that matter to you, \n",
    "# you can see what's being filtered out by the blacklist by running this pring\n",
    "print(stem_add_blacklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character or relationship tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get characters or relationships\n",
    "tag_list = []\n",
    "for u in urls:\n",
    "    time.sleep(delay)\n",
    "    response = requests.get(u, headers = headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    job_elements = soup.find_all(\"ul\", class_=\"tags commas\") \n",
    "    \n",
    "    for j in job_elements:\n",
    "        tags = j.find_all(\"li\", class_=\"relationships\") # leave as-is if you want relationships, comment out for characters\n",
    "        #tags = j.find_all(\"li\", class_=\"characters\") # leave as-is if you want relationships, uncomment for characters\n",
    "        \n",
    "        for t in tags:\n",
    "            tx = t.text\n",
    "            if tx not in blacklist:\n",
    "                found = False\n",
    "                for i in range(len(tag_list)):\n",
    "                    if tx in tag_list[i].get_equivs():\n",
    "                        tag_list[i].increment()\n",
    "                        found = True\n",
    "                if not found:\n",
    "                    new_tag = ao3_tag(t=tx)\n",
    "                    tag_list.append(new_tag)\n",
    "\n",
    "print(len(tag_list))\n",
    "\n",
    "sortedlist = sorted(tag_list, key=lambda ao3_tag: ao3_tag.count, reverse = True)\n",
    "\n",
    "for i in range(num_tags_wanted):\n",
    "    sortedlist[i].get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fic_count = 0\n",
    "word_sum = 0\n",
    "for u in urls:\n",
    "    time.sleep(delay)\n",
    "    response = requests.get(u, headers = headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    job_elements = soup.find_all(\"dl\", class_=\"stats\") \n",
    "    \n",
    "    for j in job_elements:\n",
    "        tags = j.find_all(\"dd\", class_=\"words\") \n",
    "        \n",
    "        for t in tags:\n",
    "            tx = int(re.sub(',','',t.text))\n",
    "            fic_count = fic_count + 1\n",
    "            word_sum = word_sum + tx\n",
    "\n",
    "print(\"Total words in all included fics: \"+str(word_sum))\n",
    "print(\"Avg words per fic: \"+ str(word_sum / fic_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
